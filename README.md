# Kernel-AutoValidator

**Kernel-AutoValidator** is a research prototype for **automatic, reference-free correctness validation of GPU kernels**, with a particular focus on CUDA tensor kernels generated by transcompilers and compiler optimization frameworks.

Unlike traditional kernel testing approaches that rely on manually implemented reference operators, Kernel-AutoValidator **statically infers kernel semantics** from memory access patterns and symbolic index expressions, and automatically generates executable test harnesses to perform end-to-end validation at scale.

---

## Motivation

Modern deep learning systems require developers and compilers to generate large numbers of low-level kernels targeting heterogeneous accelerators (e.g., NVIDIA GPUs, AMD GPUs, NPUs).
However, **validating the correctness of these kernels remains a major bottleneck**:

* Reference implementations are often unavailable for **fused or non-standard operators**
* Manually writing reference code is **time-consuming and error-prone**
* Differential testing does not scale across architectures or kernel variants

Kernel-AutoValidator addresses this problem by enabling:

> **Automatic kernel-level correctness validation without reference implementations**

---

## Key Idea

Kernel-AutoValidator is based on the observation that **kernel semantics are implicitly encoded in memory access behavior**.

By statically analyzing:

* memory read/write patterns,
* symbolic index expressions involving `threadIdx`, `blockIdx`, and `blockDim`,
* and kernel parameter types,

the system can automatically infer:

* **Buffer roles** (input / output / in-place)
* **Safe tensor shapes** that cover all memory accesses
* **Execution configurations** (grid and block dimensions)
* **Kernel invariants** for reference-free correctness checking


## Features

* ğŸ” **Static memory access analysis**

  * Extracts read/write sets from CUDA kernels
  * Infers buffer roles without annotations

* ğŸ“ **Symbolic shape inference**

  * Derives valid tensor shapes from index expressions
  * Prevents out-of-bounds accesses during testing

* ğŸš€ **Automatic execution configuration**

  * Infers grid/block dimensions compatible with kernel semantics

* ğŸ§ª **Reference-free correctness checking**

  * Property-based validation (determinism, dependency, numerical sanity)
  * Supports complex fused kernels

* ğŸ” **Native and transpiled kernel support**

  * Applicable to handwritten CUDA code and compiler-generated kernels

---

## Repository Structure

```text
kernel-autovalidator/
â”œâ”€â”€ examples/              # Example CUDA kernels
â”œâ”€â”€ validator/
â”‚   â”œâ”€â”€ parser/            # Lightweight CUDA parsing
â”‚   â”œâ”€â”€ analysis/          # Static semantic inference
â”‚   â”œâ”€â”€ codegen/           # Host-side test generation
â”‚   â”œâ”€â”€ runtime/           # Kernel execution and checking
â”‚   â””â”€â”€ validate.py        # End-to-end validation entry
â”œâ”€â”€ scripts/               # Utility scripts
â””â”€â”€ README.md
```

---

## Validation Workflow

Given a CUDA kernel, Kernel-AutoValidator performs the following steps:

1. **Parse kernel code**

   * Identify kernel parameters and memory accesses

2. **Infer buffer roles**

   * Classify parameters as input, output, or in-place buffers

3. **Analyze symbolic index expressions**

   * Build symbolic representations of thread-level indexing

4. **Infer tensor shapes and execution configuration**

   * Determine safe tensor sizes and grid/block dimensions

5. **Generate host-side test harness**

   * Allocate buffers, initialize inputs, launch kernel

6. **Execute and validate**

   * Perform reference-free correctness checks based on inferred invariants

---

## Example

For the following kernel:

```cpp
__global__ void vec_add(float* A, float* B, float* C) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  C[i] = A[i] + B[i];
}
```

Kernel-AutoValidator automatically infers:

* `A`, `B` as **input buffers**
* `C` as an **output buffer**
* Tensor shape: `N = gridDim.x * blockDim.x`
* Execution configuration: `blockDim = 256`, `gridDim = ceil(N / 256)`
* Correctness properties: determinism, absence of NaNs, input-dependence

No reference implementation is required.

---

## Scope and Limitations

Kernel-AutoValidator is designed for **data-parallel tensor kernels** commonly used in deep learning workloads.

Current limitations include:

* Limited support for kernels with complex control flow (e.g., data-dependent branches)
* Conservative shape inference for irregular memory access patterns
* No performance validation (correctness only)

These limitations are consistent with the design goal of **sound and scalable kernel-level validation**.

---

## Research Context

Kernel-AutoValidator is intended to support research on:

* Neuralâ€“symbolic program synthesis
* Transcompilation across heterogeneous accelerators
* Compiler-generated kernel validation
* Reference-free testing for fused tensor operators

It is particularly useful in compiler pipelines where large numbers of kernel variants are automatically generated.
